{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import SemiSupervisedMNIST\n",
    "from models import Discriminator, Generator\n",
    "from losses import DiscriminatorLoss, GeneratorLoss\n",
    "from metrics import AverageAccuracy, FakeAccuracy, Loss, ClassAccuracy, RunTime\n",
    "from history import History\n",
    "from trainers import GAN_Trainer, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "np.random.seed(876)\n",
    "pd.np.random.seed(876)\n",
    "torch.manual_seed(876)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(876)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Option 1] Load MNIST dataset for semi-supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_size = 100\n",
    "distribution = torch.distributions.normal.Normal(0, 1)     \n",
    "label_encoding = {n: n for n in range(9)}\n",
    "label_encoding['fake'] = len(label_encoding)\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    torchvision.transforms.Lambda(lambda x: x.flatten().float())\n",
    "])         \n",
    "train_dataset = SemiSupervisedMNIST(num_labeled=10,\n",
    "                                    noise_size=noise_size,\n",
    "                                    distribution=distribution,\n",
    "                                    label_encoding=label_encoding,\n",
    "                                    root='./mnist_data', train=True, transform=transforms, download=False)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./mnist_data', train=False, transform=transforms, download=False)\n",
    "test_dataset.label_encoding = label_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Option 2] Load MREO haptics dataset\n",
    "* Compact dataset (1 GB) (can be used to compute tables 1, 2, 3, 4, and 6): https://goo.gl/WiqSjJ\n",
    "* https://github.com/Healthcare-Robotics/mr-gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "mel = True\n",
    "modalities = ['temperature', 'force0', 'force1', 'contact']\n",
    "\n",
    "data_dir = './haptics_data/'\n",
    "data_files = os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = {}\n",
    "materials = []\n",
    "for file in data_files:\n",
    "    path = os.path.join(data_dir, file)\n",
    "    material = file.split('_')[2]\n",
    "    materials.append(material)\n",
    "    with open(path, 'rb') as pkl_file:\n",
    "        c = pickle.load(pkl_file, encoding='latin1')\n",
    "        raw[material] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_material = []\n",
    "d_obj = []\n",
    "d_obj_sample_num = []\n",
    "d_data = collections.defaultdict(list)\n",
    "for material in materials:\n",
    "    c = raw[material]\n",
    "    for obj in c:\n",
    "        for obj_sample_num in range(len(c[obj]['temperature'])):\n",
    "            d_material.append(material)\n",
    "            d_obj.append(obj)\n",
    "            d_obj_sample_num.append(obj_sample_num)\n",
    "            \n",
    "            for modality in modalities:\n",
    "                modality_data = c[obj][modality][obj_sample_num]\n",
    "                if modality is 'contact' and mel:\n",
    "                    S = librosa.feature.melspectrogram(np.array(modality_data), sr=48000, n_mels=128)\n",
    "                    # Convert to log scale (dB)\n",
    "                    log_S = librosa.amplitude_to_db(S, ref=np.max)\n",
    "                    d_data[modality].append(log_S.flatten())\n",
    "                else:\n",
    "                    d_data[modality].append(modality_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(material=d_material, obj=d_obj, obj_sample_num=d_obj_sample_num)\n",
    "for modality in modalities:\n",
    "    d[modality] = d_data[modality]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=d)\n",
    "data_i = list(range(len(df)))\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised learning dataset example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "train_i, test_i = train_test_split(data_i, test_size=0.2, stratify=df['material'].iloc[data_i])\n",
    "\n",
    "# Scale data to zero mean and unit variance\n",
    "for modality in modalities:\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    train_norm = scaler.fit_transform(np.stack(df[modality].iloc[train_i].values))\n",
    "    test_norm = scaler.transform(np.stack(df[modality].iloc[test_i].values))\n",
    "    \n",
    "    df[modality].iloc[train_i] = train_norm.tolist()\n",
    "    df[modality].iloc[test_i] = test_norm.tolist()\n",
    "    \n",
    "label_encoding = {m: i for i, m in enumerate(list(df['material'].unique()))}\n",
    "\n",
    "datasets_i = {'train': train_i, 'test': test_i}\n",
    "datasets = {l: MaterialDataset(modalities, label_encoding, df=df.iloc[i].reset_index().rename(columns={'index': 'sample_id'})) for l, i in datasets_i.items()}\n",
    "phases = list(datasets.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semi-supervised leraning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_unlabeled = 0.25\n",
    "noise_size = 100\n",
    "distribution = torch.distributions.normal.Normal(0, 1)\n",
    "\n",
    "label_encoding = {m: i for i, m in enumerate(list(df['material'].unique()))}\n",
    "label_encoding['fake'] = len(label_encoding)\n",
    "\n",
    "datasets = {'train': SemiSupervisedMaterialDataset(modalities, \n",
    "                                                   label_encoding,\n",
    "                                                   df=df.iloc[train_i].reset_index().rename(columns={'index': 'sample_id'}), \n",
    "                                                   percent_unlabeled=percent_unlabeled,\n",
    "                                                   noise_size=noise_size,\n",
    "                                                   distribution=distribution),\n",
    "            'test': MaterialDataset(modalities, label_encoding, df.iloc[test_i].reset_index().rename(columns={'index': 'sample_id'}))\n",
    "           }\n",
    "phases = list(datasets.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {'train': train_dataset,\n",
    "            'test': test_dataset}\n",
    "phases = list(datasets.keys())\n",
    "dataloader_params = {'train': {'batch_size': 64, 'shuffle': True, 'num_workers': 8, 'pin_memory': use_cuda},\n",
    "                    'test':  {'batch_size': 1, 'shuffle': False, 'num_workers': 8, 'pin_memory': use_cuda}\n",
    "                    }\n",
    "dataloaders = {l: data.DataLoader(d, **dataloader_params[l]) for l, d in datasets.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised learning example with fully-connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model_NN(datasets['train'].shape[0], datasets['train'].shape[1])\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0006, betas=(0.5, 0.999))\n",
    "\n",
    "metrics = [AverageAccuracy(), \n",
    "           ClassAccuracy(len(label_encoding)), \n",
    "           Loss(nn.CrossEntropyLoss(), name='CrossEntropy'), \n",
    "           Loss(nn.MSELoss(), name='MSE', output_transform=lambda y_pred, y: (y_pred, to_onehot(y, len(label_encoding)).float())),\n",
    "           RunTime() \n",
    "          ]\n",
    "\n",
    "viz_params = {\n",
    "    'to_viz': True,\n",
    "    'bands': False\n",
    "}\n",
    "history = History(metrics=metrics, viz_params=viz_params, phases=list(datasets.keys()))\n",
    "t = Trainer(model=net,\n",
    "            dataloaders=dataloaders, \n",
    "            optimizer=optimizer, \n",
    "            criterion=nn.CrossEntropyLoss(), \n",
    "            history=history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semi-supervised learning GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = {\n",
    "    'D': Discriminator(datasets['train'].shape[0], datasets['train'].shape[1], feature_matching=True, leaky=0.2),\n",
    "    'G': Generator(noise_size, datasets['train'].shape[0])\n",
    "}\n",
    "optimizers = {\n",
    "    'D': optim.Adam(nets['D'].parameters(), lr=0.0006, betas=(0.5, 0.999)),\n",
    "    'G': optim.Adam(nets['G'].parameters(), lr=0.0006, betas=(0.5, 0.999)),\n",
    "}\n",
    "    \n",
    "criterions = {\n",
    "    'D': DiscriminatorLoss(return_all=True),\n",
    "    'G': GeneratorLoss()\n",
    "}\n",
    "metrics = {\n",
    "    'D': [AverageAccuracy(),\n",
    "          FakeAccuracy(output_transform=lambda x: (x[0], x[1].long())),\n",
    "          Loss(criterions['D'], name='loss_D'),\n",
    "          Loss(criterions['D'], name='loss_labeled'),\n",
    "          Loss(criterions['D'], name='loss_unlabeled'),\n",
    "          ClassAccuracy(len(label_encoding)),\n",
    "          RunTime()],\n",
    "    'G': [FakeAccuracy(output_transform=lambda x: (x[0], x[1].long())),\n",
    "          Loss(criterions['G'], name='loss_G'),\n",
    "          RunTime()],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_params = {\n",
    "    'D': {\n",
    "        'to_viz': True,\n",
    "        'bands': False\n",
    "    },\n",
    "    'G': {\n",
    "        'to_viz': False,\n",
    "        'bands': False\n",
    "    }\n",
    "}\n",
    "history = {\n",
    "    'D': History(metrics=metrics['D'], viz_params=viz_params['D'], phases=list(datasets.keys()), verbose=0),\n",
    "    'G': History(metrics=metrics['G'], viz_params=viz_params['G'], phases=list(datasets.keys()), verbose=0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = GAN_Trainer(model=nets,\n",
    "                dataloaders=dataloaders, \n",
    "                optimizer=optimizers,\n",
    "                criterion=criterions, \n",
    "                history=history,\n",
    "                device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook\n",
    "from bokeh.plotting import show\n",
    "from bokeh.layouts import gridplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_notebook()\n",
    "grid_D = history['D'].viz()\n",
    "history['D'].viz_handle = show(gridplot(grid_D), notebook_handle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t.load('checkpoints/checkpoint_50.pt')\n",
    "t.run(max_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history['G'].to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('discriminator history:')\n",
    "print(history['D'].to_df())\n",
    "print('-' * 100)\n",
    "print('generator history:')\n",
    "print(history['G'].to_df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
